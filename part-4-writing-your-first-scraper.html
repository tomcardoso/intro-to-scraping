<html>
	<head>
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css">
		<link rel="stylesheet" href="lib/css/zenburn.css">
		<style>
			.reveal h1 {
				text-transform: none;
				line-height: 1;
			}
			.reveal ul {
				margin: 0;
			}
			.reveal li {
				list-style-type: none;
			}
			.reveal p {
				margin: 0;
				margin-bottom: 0.5em;
			}
			.reveal pre {
				box-shadow: none;
			}
			.reveal pre code {
				padding: 25px;
			}
		</style>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<section>
					<h1>Writing your first scraper with <code>rvest</code></h1>
				</section>

				<section>
					<p>Let's get into the R portion of the class. Please let me know if I'm going too slow (or too fast!).</p>
				</section>

				<section>
					<p>Robust scraping of the sort we're about to attempt can be frustrating. It's a lot of write, run, tweak, repeat. Whenever I'm coding for work, I spend most of my day feeling like this:</p>
				</section>

				<section>
					<img src="img/b7e.jpg" alt="">
				</section>

				<section>
					<p>Or like this</p>
					<img src="img/ihave.jpg" alt="">
				</section>

				<section>
					<p>Or like this…</p>
					<img src="img/wpengine-meme.jpg" alt="">
				</section>

				<section>
					<p>That's okay! The most important thing to remember when dealing with any kind of coding problem: someone else has already had this problem, asked about it online, and got an answer, which means… you can Google it! Or ask about it on forums such as StackOverflow.</p>
				</section>

				<section>
					<p>Before we get to the good stuff, let's talk about the <strong>tidyverse</strong> a little bit. It's a community and set of coding packages for the R language that make analysis <strong>monumentally</strong> easier. <code>rvest</code> is part of the tidyverse.</p>
				</section>

				<section>
					<p>There's a lot to know about the tidyverse, but the most important thing we'll need to know for this class is the <strong>pipe</strong> function. It looks like this:</p>
					<h4><code>%>%</code></h4>
				</section>

				<section>
					<p>It, together with other parts of the tidyverse, allows you to turn this…</p>
					<pre>
						<code data-trim>
							for (n in unique(d$x)) {
								subd <- d[d$x == n]
							…
						</code>
					</pre>
					Into this:
					<pre>
						<code data-trim>
							d %>%
								group_by(x) %>%
							…
						</code>
					</pre>
				</section>

				<section>
					<p>All the pipe does is tell R: "take the stuff from the left-hand side of the operation, and apply it to the stuff on the right."</p>
				</section>

				<section>
					<p>Let's use tidyverse packages together to get a feel for how this works.</p>
				</section>

				<section>
					<p><strong>tidyverse.R</strong></p>
				</section>

				<section>
					<p>A slightly more advanced example: <strong>sunshine.R</strong></p>
				</section>

				<section>
					<p>Now let's load the <code>rvest</code> package and get down to it.</p>
				</section>

				<section>
					<p>At its core, <code>rvest</code> hinges on one command: <code>read_html()</code>. That command tells <code>rvest</code> which page it should fetch. Let's try it out.</p>
				</section>

				<section>
					<p><strong>procurement.R</strong></p>
				</section>

				<section>
					<p>Building that index of URLs is a common scraping tactic — build a scraper, grab a list of URLs, and then build a second scraper that consumes those URLs.</p>
				</section>

				<section>
					<p>The only issue with this approach is that you have no guarantee the HTML structure will be consistent page to page. I originally planned to show you the nested scraper working on the Treasury Board website, but the HTML structure of the tables was all over the place. A great real-life example of this variability!</p>
				</section>

				<section>
					<p>Before reaching for <code>rvest</code>, you should make sure you actually need it. Here's an example: <a href="# https://www.canada.ca/en/health-canada/services/drugs-health-products/drug-products/prescription-drug-list/list.html"># https://www.canada.ca/en/health-canada/services/drugs-health-products/drug-products/prescription-drug-list/list.html</a></p>
				</section>

				<section>
					<p>Because this page is a single table, you can actually just select the table with your cursor, copy it, and paste it into Excel!</p>
				</section>

				<section>
					<p><code>rvest</code> won't always work. High-traffic sites are wise to our scraping ways, and are fairly scrape-proof. Let's take a look at the source for Facebook and see for ourselves.</p>
				</section>

				<section>
					<p>Even on fairly easy-to-scrape sites, you'll want to be careful not to scrape too aggressively. <code>rvest</code> will want to go as fast as possible, but that could get your IP address automatically banned by the server.</p>
				</section>

				<section>
					<p>That's where throttling comes in. Let's build a basic scraper with some throttling functionality.</p>
				</section>

				<section>
					<p><strong>globeandmail.R</strong></p>
				</section>

				<section>
					<p>One important mantra to follow when scraping: <em>never</em> manipulate or clean up your data within the scrape process. It's much better to get a raw scrape of a page, then clean it up either in a different file or a different variable. Depending on your data, you may have thousands of pages to scrape — you don't want to have to scrape it multiple times if at all possible.</p>
				</section>

				<section>
					<p>A corollary of this rule is that you should always err on the side of collecting more rather than less data. Put another way: if the web page you're scraping has a series of fields (let's say date, name, address, age, and postal code), you may think you don't need the postal code because you already have the address. You should <strong>still scrape it</strong>. You never know what you'll end up needing later.</p>
				</section>

				<section>
					<p>Finally, as much as possible, try to make your scrape reproducible. That is, try to design it such that you can re-run it again two months later with minimal tweaking. <code>rvest</code> does a good job of offering reproducibility out of the box, but just beware that you may end up revisiting an old scraper two years later.</p>
				</section>

				<section>
					<p>Now that that's all done, let's pick a website together and write a new scraper from scratch. This could go off the rails, so apologies in advance…</p>
				</section>

				<section>
					<p><strong>newscrape.R</strong></p>
				</section>

				<section>
					<p>Finally, in some cases you may want to scrape a website by partly filling in forms or submitting data. For example, say you want to scrape a web page that requires you to paginate through a list (<a href="http://www1.canada.ca/consultingcanadians/page/search?lang=en&keywords=&start=1&type=all&year=0&subjectid=0&departmentid=73&submit=Apply">here's an example</a>)…</p>
				</section>

				<section>
					<p>You can automate the clicking of that "next" button and filling out of forms with a package called RSelenium. It's very powerful, but also very involved, and a bit outside the scope of this course.</p>
				</section>

				<section>
					<p>If you're curious, I've included a link to an RSelenium tutorial in the online course notes.</p>
				</section>

				<section>
					<p>15 minute break!</p>
					<img src="img/7Dl2.gif" alt="">
				</section>

			</div>
		</div>
		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>
		<script>
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
				]
			});
		</script>
	</body>
</html>
